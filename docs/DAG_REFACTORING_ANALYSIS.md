# An√°lise e Refatora√ß√£o do DAG: inmet_data_to_snowflake_dbt_etl-loop-years

## üìä Problema Identificado

### ‚ùå Abordagem Anterior (Problem√°tica)

O c√≥digo original tinha uma **arquitetura h√≠brida incorreta**:

```python
def generate_copy_tasks(**context):
    # Criava SQLExecuteQueryOperator DENTRO da fun√ß√£o Python
    for filename in files:
        copy_task = SQLExecuteQueryOperator(...)  # ‚ùå Criado em runtime
        # Tentava fazer chaining, mas n√£o executava
```

**Problemas:**
1. ‚úó Operadores SQL criados **em tempo de execu√ß√£o** (quando `generate_copy_tasks` rodava)
2. ‚úó Airflow requer que o **DAG seja definido estaticamente** no parse time
3. ‚úó Tasks criadas dinamicamente **n√£o eram executadas** automaticamente
4. ‚úó Dependencies (>>) definidas dentro da fun√ß√£o n√£o funcionavam corretamente

### üîç Fluxo Anterior (Quebrado)

```
Ano 2000-2025 (loop):
  ‚îú‚îÄ list_s3_files_{year}        ‚Üê ‚úÖ Executava boto3, retornava lista via XCom
  ‚îú‚îÄ generate_copy_tasks_{year}   ‚Üê ‚ö†Ô∏è  Executava Python, CRIAVA tasks
  ‚îî‚îÄ [Tasks COPY criadas mas N√ÉO executadas] ‚ùå
```

**Resultado:** Os arquivos eram listados, mas o COPY INTO nunca executava!

---

## ‚úÖ Solu√ß√£o Implementada

### Refatora√ß√£o com TaskFlow API + Dynamic Task Mapping

Implementei a abordagem moderna do Airflow 2.3+ usando:

1. **@task decorators** - TaskFlow API para fun√ß√µes Python
2. **Dynamic Task Mapping** - `.expand()` para processar m√∫ltiplos itens
3. **Proper XCom handling** - Passagem autom√°tica de dados entre tasks

### üéØ Novo Fluxo (Correto)

```
create_file_format
      |
      v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ano 2000                                   ‚îÇ
‚îÇ  ‚îú‚îÄ list_s3_files(2000)                     ‚îÇ
‚îÇ  ‚îÇ    ‚îî‚îÄ Retorna: [{file1}, {file2}, ...]   ‚îÇ
‚îÇ  ‚îî‚îÄ copy_file_to_snowflake.expand()         ‚îÇ
‚îÇ       ‚îú‚îÄ Task[0]: COPY file1  ‚úì             ‚îÇ
‚îÇ       ‚îú‚îÄ Task[1]: COPY file2  ‚úì             ‚îÇ
‚îÇ       ‚îî‚îÄ Task[n]: COPY fileN  ‚úì             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      |
      v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ano 2001                                   ‚îÇ
‚îÇ  ‚îú‚îÄ list_s3_files(2001)                     ‚îÇ
‚îÇ  ‚îî‚îÄ copy_file_to_snowflake.expand()         ‚îÇ
‚îÇ       ‚îî‚îÄ [Tasks din√¢micas por arquivo]      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      |
      v
    ...
      |
      v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Ano 2025                                   ‚îÇ
‚îÇ  ‚îú‚îÄ list_s3_files(2025)                     ‚îÇ
‚îÇ  ‚îî‚îÄ copy_file_to_snowflake.expand()         ‚îÇ
‚îÇ       ‚îî‚îÄ [Tasks din√¢micas por arquivo]      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîß Mudan√ßas T√©cnicas Implementadas

### 1. Import do TaskFlow API

```python
from airflow.decorators import task
```

### 2. Fun√ß√£o `list_s3_files` Refatorada

**Antes:**
```python
def list_s3_files(bucket, base_prefix, year, **context):
    # Retornava string com newlines
    return '\n'.join(files) if files else ""
```

**Depois:**
```python
@task
def list_s3_files(bucket: str, base_prefix: str, year: int):
    # Retorna lista de dicts com metadados
    return [
        {'filename': 'file1.CSV', 'year': 2000, 'size': 12345},
        {'filename': 'file2.CSV', 'year': 2000, 'size': 67890},
        ...
    ]
```

**Benef√≠cios:**
- ‚úÖ Type hints para melhor documenta√ß√£o
- ‚úÖ Retorna estrutura de dados rica (dict) em vez de string
- ‚úÖ Decorator @task gerencia XCom automaticamente
- ‚úÖ Metadados (size, year) passados para pr√≥xima task

### 3. Nova Fun√ß√£o `copy_file_to_snowflake`

```python
@task
def copy_file_to_snowflake(file_info: dict):
    """Execute Snowflake COPY INTO para um √∫nico arquivo CSV."""
    from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
    
    year = file_info['year']
    filename = file_info['filename']
    
    # Constr√≥i e executa SQL
    sql = f"""
        COPY INTO {FULLY_QUALIFIED_TABLE}
        FROM (...)
        FROM @{SNOWFLAKE_DATABASE}.{SNOWFLAKE_STAGE}.STAGE_RAW/{year}/{filename}
        (FILE_FORMAT => '{FULLY_QUALIFIED_FILE_FORMAT}')
    """
    
    hook = SnowflakeHook(snowflake_conn_id=SNOWFLAKE_CONN_ID)
    result = hook.run(sql, autocommit=True)
    
    return {'filename': filename, 'status': 'success', ...}
```

**Caracter√≠sticas:**
- ‚úÖ Processa **um arquivo por vez** (task isolada)
- ‚úÖ Usa SnowflakeHook para execu√ß√£o direta
- ‚úÖ Logging detalhado por arquivo
- ‚úÖ Retorna m√©tricas de processamento
- ‚úÖ Tratamento de erro isolado por arquivo

### 4. Dynamic Task Mapping com `.expand()`

**C√≥digo:**
```python
for year in YEARS:
    # Lista arquivos (retorna list[dict])
    files_list = list_s3_files(bucket=bucket_name, base_prefix=initial_prefix, year=year)
    
    # MAGIC: .expand() cria UMA TASK PARA CADA ITEM DA LISTA
    copy_results = copy_file_to_snowflake.expand(file_info=files_list)
    
    # Dependency chaining
    create_file_format >> files_list >> copy_results
```

**Como funciona `.expand()`:**
- Airflow pega a lista retornada por `list_s3_files`
- Cria **dinamicamente** uma task `copy_file_to_snowflake` para cada item
- Tasks s√£o nomeadas automaticamente: `copy_file_to_snowflake[0]`, `copy_file_to_snowflake[1]`, etc.
- Cada task recebe **um dict** do array como par√¢metro `file_info`

---

## üìà Benef√≠cios da Refatora√ß√£o

### ‚úÖ Execu√ß√£o Garantida
- **Antes:** Tasks criadas mas n√£o executadas
- **Depois:** Cada arquivo gera uma task que executa automaticamente

### ‚úÖ Visibilidade no Airflow UI
- **Antes:** Apenas `generate_copy_tasks` aparecia (sem detalhes)
- **Depois:** Cada arquivo tem sua pr√≥pria task vis√≠vel no Graph View

### ‚úÖ Paraleliza√ß√£o (Potencial)
- **Antes:** Imposs√≠vel paralelizar
- **Depois:** Tasks podem rodar em paralelo (configur√°vel via `max_active_tasks`)

### ‚úÖ Retry Isolado
- **Antes:** Falha em um arquivo quebrava tudo
- **Depois:** Retry granular por arquivo

### ‚úÖ Logging Estruturado
- Cada arquivo tem seu pr√≥prio log
- M√©tricas individuais: tempo de processamento, linhas carregadas
- Rastreabilidade completa

### ‚úÖ Type Safety
- Type hints para par√¢metros
- Valida√ß√£o autom√°tica pelo Airflow

---

## üéØ Verifica√ß√£o do Fluxo

### Como garantir que CADA arquivo √© processado:

1. **list_s3_files** retorna: `[{file1}, {file2}, {file3}]`
2. **.expand()** cria automaticamente:
   - `copy_file_to_snowflake[0]` ‚Üí processa `{file1}`
   - `copy_file_to_snowflake[1]` ‚Üí processa `{file2}`
   - `copy_file_to_snowflake[2]` ‚Üí processa `{file3}`
3. **Airflow scheduler** executa cada task
4. **SnowflakeHook** executa o COPY INTO para cada arquivo

### Diagrama de Depend√™ncias:

```
create_file_format
        |
        v
  list_s3_files_2000  ‚Üí  [array com 5 arquivos]
        |
        v
copy_file_to_snowflake[0]  ‚Üê arquivo 1
copy_file_to_snowflake[1]  ‚Üê arquivo 2
copy_file_to_snowflake[2]  ‚Üê arquivo 3
copy_file_to_snowflake[3]  ‚Üê arquivo 4
copy_file_to_snowflake[4]  ‚Üê arquivo 5
        |
        v
  list_s3_files_2001  ‚Üí  [array com 3 arquivos]
        |
        v
copy_file_to_snowflake[0]  ‚Üê arquivo 1
copy_file_to_snowflake[1]  ‚Üê arquivo 2
copy_file_to_snowflake[2]  ‚Üê arquivo 3
```

---

## üìù Exemplo de Execu√ß√£o

### Logs Esperados:

```
[list_s3_files_2006]
[YEAR 2006] Starting S3 file listing...
[YEAR 2006] Found file: INMET_CO_GO_A002_GOIANIA.CSV (size: 1234567 bytes)
[YEAR 2006] Found file: INMET_CO_GO_A011_SAO_SIMAO.CSV (size: 987654 bytes)
[YEAR 2006] S3 listing completed: 2 CSV files found

[copy_file_to_snowflake[0]]
[YEAR 2006] Processing file: INMET_CO_GO_A002_GOIANIA.CSV
[YEAR 2006] S3 path: inmet/2006/INMET_CO_GO_A002_GOIANIA.CSV
[YEAR 2006] File size: 1234567 bytes
[YEAR 2006] File processed successfully in 2.45s
[YEAR 2006] Query result: [('success', 8760)]  ‚Üê linhas carregadas

[copy_file_to_snowflake[1]]
[YEAR 2006] Processing file: INMET_CO_GO_A011_SAO_SIMAO.CSV
[YEAR 2006] S3 path: inmet/2006/INMET_CO_GO_A011_SAO_SIMAO.CSV
[YEAR 2006] File size: 987654 bytes
[YEAR 2006] File processed successfully in 1.89s
[YEAR 2006] Query result: [('success', 7320)]
```

---

## üöÄ Pr√≥ximos Passos (Melhorias Futuras)

### 1. Paraleliza√ß√£o
```python
copy_results = copy_file_to_snowflake.expand(
    file_info=files_list,
    max_active_tasks=10  # Processar 10 arquivos simultaneamente
)
```

### 2. Captura de M√©tricas do Snowflake
```python
# Parsear o resultado do COPY INTO
result = hook.run(sql)
rows_loaded = int(result[0][1])  # Extrair do result set
return {'rows_loaded': rows_loaded, 'status': 'success'}
```

### 3. Task Group por Ano
```python
from airflow.utils.task_group import TaskGroup

with TaskGroup(f"year_{year}") as year_group:
    files_list = list_s3_files(...)
    copy_results = copy_file_to_snowflake.expand(...)
```

### 4. Tratamento de Erros Avan√ßado
```python
@task(retries=3, retry_delay=timedelta(minutes=5))
def copy_file_to_snowflake(file_info: dict):
    try:
        # ... COPY INTO
    except Exception as e:
        logger.error(f"Failed to process {filename}: {e}")
        # Enviar alerta, registrar falha, etc.
        raise
```

---

## üìö Refer√™ncias

- [Airflow TaskFlow API](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)
- [Dynamic Task Mapping](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/dynamic-task-mapping.html)
- [Snowflake Hook](https://airflow.apache.org/docs/apache-airflow-providers-snowflake/stable/_api/airflow/providers/snowflake/hooks/snowflake/index.html)

---

**Data:** 2025-10-15  
**Autor:** GitHub Copilot  
**Status:** ‚úÖ Implementado e Testado
